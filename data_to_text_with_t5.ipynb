{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-base\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class webnlg(Dataset):\n",
    "    def __init__(self, tokenizer, path_data,\n",
    "                 save_df=True, save_dataset=True,\n",
    "                 load_path_df=\"\", load_path_dataset=\"\",\n",
    "                 keep_df=True, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if load_path_df:\n",
    "            self.df = pd.read_csv(load_path_df)\n",
    "        elif load_path_dataset and not keep_df:\n",
    "            pass # no need to load nor compute df\n",
    "        else:\n",
    "            self.df = None\n",
    "            self.make_df(path_data, save_df)\n",
    "\n",
    "        if load_path_dataset:\n",
    "            self.dataset = np.load(load_path_dataset, allow_pickle=True)\n",
    "        else:\n",
    "            self.dataset = None\n",
    "            self.make_dataset(path_data, save_dataset)\n",
    "        if not keep_df:\n",
    "            del self.df\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def clean_text(self, text, type=\"\"):\n",
    "        if type == \"RDF\":\n",
    "            text = \"<S> \" + text\n",
    "            text = text.replace(\"|\", \"<P>\", 1)\n",
    "            text = text.replace(\"|\", \"<O>\", 1)\n",
    "        text = text.replace('\"', \"\")\n",
    "        text = text.replace(\"_\", \" \")\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        for letters in re.findall(r\"[a-z][A-Z][a-z]\", text):\n",
    "            text = re.sub(r\"[a-z][A-Z][a-z]\", letters[0] + \" \" + letters[1:], text)\n",
    "        return text\n",
    "\n",
    "    def make_df(self, path_data, save_df=True):\n",
    "        if self.verbose:\n",
    "            print(\"Make DataFrame\")\n",
    "        t_total = time.time()\n",
    "        count_1 = 0\n",
    "        total_count_1 = len(os.listdir(path_data))\n",
    "        df = pd.DataFrame(columns=[\"RDF\", \"sequence\", \"RDF_seq_tokenized\"])\n",
    "        for ktriples in os.listdir(path_data):\n",
    "            count_1 += 1\n",
    "            path_ktriples = os.path.join(path_data, ktriples)\n",
    "            t_sub = time.time()\n",
    "            count_2 = 0\n",
    "            total_count_2 = len(os.listdir(path_ktriples))\n",
    "            for file in os.listdir(path_ktriples):\n",
    "                tree = ET.parse(os.path.join(path_ktriples, file))\n",
    "                root = tree.getroot()\n",
    "                for entry in root[0]:\n",
    "                    triple_all = \"Generate in English:\"\n",
    "                    for modifiedtripletset in entry.findall(\"modifiedtripleset\"):\n",
    "                        for mtriple in modifiedtripletset.findall(\"mtriple\"):\n",
    "                            triple = self.clean_text(mtriple.text, \"RDF\")\n",
    "                            triple_all += \" \" + triple\n",
    "                    triple_all_tokenized = tokenizer.encode_plus(triple_all,\n",
    "                                                                 return_tensors='pt',\n",
    "                                                                 max_length=512,\n",
    "                                                                 padding=\"max_length\",\n",
    "                                                                 truncation=True)\n",
    "                    for lex in entry.findall(\"lex\"):\n",
    "                        seq = self.clean_text(lex.text)\n",
    "                        seq_tokenized = tokenizer.encode_plus(seq,\n",
    "                                                              return_tensors='pt',\n",
    "                                                              max_length=512,\n",
    "                                                              padding=\"max_length\",\n",
    "                                                              truncation=True)\n",
    "                        df.loc[len(df)] = [triple_all, seq, [triple_all_tokenized, seq_tokenized]]\n",
    "                if self.verbose:\n",
    "                    count_2 += 1\n",
    "                    print(\"\\t{}/{} - {:.1f}% - elapsed : {:.1f}s - total : {:.1f}s\".format(count_1,\n",
    "                                                                                         total_count_1,\n",
    "                                                                                         count_2 / total_count_2 * 100,\n",
    "                                                                                         time.time() - t_sub,\n",
    "                                                                                         time.time() - t_total),\n",
    "                          end=\"\\r\")\n",
    "            if self.verbose:\n",
    "                print(\"\")\n",
    "        if save_df:\n",
    "            if self.verbose:\n",
    "                print(\"\")\n",
    "                print(\"Saving... \",end=\"\")\n",
    "            df.to_csv(path_data + \"_dataframe.csv\", index=False)\n",
    "            if self.verbose:\n",
    "                print(\"DataFrame {} saved.\".format(path_data + \"_dataframe.csv\"))\n",
    "        self.df = df\n",
    "        if self.verbose:\n",
    "            print(\"\\tDone. elapsed : {:.1f}s\\n\".format(time.time()-t_total))\n",
    "\n",
    "    def make_dataset(self, path_data, save_dataset):\n",
    "        t = time.time()\n",
    "        if self.verbose:\n",
    "            print(\"Make Dataset\")\n",
    "        self.dataset = self.df[\"RDF_seq_tokenized\"].to_numpy()\n",
    "        if save_dataset:\n",
    "            if self.verbose:\n",
    "                print(\"\")\n",
    "                print(\"Saving... \",end=\"\")\n",
    "            np.save(path_data + \"_dataset.npy\", self.dataset)\n",
    "            if self.verbose:\n",
    "                print(\"Dataset {} saved.\".format(path_data + \"_dataset.npy\"))\n",
    "        if self.verbose:\n",
    "            print(\"\\tDone. elapsed : {:.1f}s\\n\".format(time.time()-t))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source, targets = self.convert_to_features(self.dataset[index])\n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "        src_mask = source[\"attention_mask\"].squeeze()\n",
    "        target_mask = targets[\"attention_mask\"].squeeze()\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "source :\n",
    "    https://shivanandroy.com/fine-tune-t5-transformer-with-pytorch/\n",
    "    https://github.com/Shivanandroy/T5-Finetuning-PyTorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "class T5_fine_tuned_dtt(torch.nn):\n",
    "    def __init__(self)\n",
    "    def train(epoch, tokenizer, model, device, loader, optimizer)\n",
    "    def validate(epoch, tokenizer, model, device, loader)\n",
    "    def T5Trainer(dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make DataFrame\n",
      "\t1/7 - 100.0% - elapsed : 14.9s - total : 14.9s\r\n",
      "\t2/7 - 100.0% - elapsed : 1.2s - total : 16.1s\r\n",
      "\t3/7 - 100.0% - elapsed : 17.4s - total : 33.5s\r\n",
      "\t4/7 - 100.0% - elapsed : 1.4s - total : 34.9s\r\n",
      "\t5/7 - 100.0% - elapsed : 14.8s - total : 49.7s\r\n",
      "\t6/7 - 100.0% - elapsed : 26.0s - total : 75.7s\r\n",
      "\t7/7 - 100.0% - elapsed : 27.4s - total : 103.1s\r\n",
      "\n",
      "Saving... DataFrame ./webnlg-dataset-master-release_v3.0/en/train_dataframe.csv saved.\n",
      "\tDone. elapsed : 230.6s\n",
      "\n",
      "Make Dataset\n",
      "\n",
      "Saving... Dataset ./webnlg-dataset-master-release_v3.0/en/train_dataset.npy saved.\n",
      "\tDone. elapsed : 6.4s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = webnlg(tokenizer, \"./webnlg-dataset-master-release_v3.0/en/train\", verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WORK IN PROGRESS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class T5_fine_tuned_dtt(torch.nn):\n",
    "    def __init__(self):\n",
    "        # let's define model parameters specific to T5\n",
    "        model_params = {\n",
    "            \"MODEL\": \"t5-base\",  # model_type: t5-base/t5-large\n",
    "            \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n",
    "            \"VALID_BATCH_SIZE\": 8,  # validation batch size\n",
    "            \"TRAIN_EPOCHS\": 3,  # number of training epochs\n",
    "            \"VAL_EPOCHS\": 1,  # number of validation epochs\n",
    "            \"LEARNING_RATE\": 1e-4,  # learning rate\n",
    "            \"MAX_SOURCE_TEXT_LENGTH\": 512,  # max length of source text\n",
    "            \"MAX_TARGET_TEXT_LENGTH\": 50,  # max length of target text\n",
    "            \"SEED\": 42,  # set seed for reproducibility\n",
    "        }\n",
    "\n",
    "        T5Trainer(\n",
    "            dataframe=df,\n",
    "            source_text=\"text\",\n",
    "            target_text=\"headlines\",\n",
    "            model_params=model_params,\n",
    "            output_dir=\"outputs\",\n",
    "        )\n",
    "\n",
    "    def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "\n",
    "        \"\"\"\n",
    "        Function to be called for training with the parameters passed from main function\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        model.train()\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
    "            y_ids = y[:, :-1].contiguous()\n",
    "            lm_labels = y[:, 1:].clone().detach()\n",
    "            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "            ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
    "            mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                decoder_input_ids=y_ids,\n",
    "                labels=lm_labels,\n",
    "            )\n",
    "            loss = outputs[0]\n",
    "\n",
    "            if _ % 10 == 0:\n",
    "                training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "                console.print(training_logger)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def validate(epoch, tokenizer, model, device, loader):\n",
    "\n",
    "        \"\"\"\n",
    "        Function to evaluate model for predictions\n",
    "\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        with torch.no_grad():\n",
    "          for _, data in enumerate(loader, 0):\n",
    "              y = data['target_ids'].to(device, dtype = torch.long)\n",
    "              ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "              mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "              generated_ids = model.generate(\n",
    "                  input_ids = ids,\n",
    "                  attention_mask = mask,\n",
    "                  max_length=150,\n",
    "                  num_beams=2,\n",
    "                  repetition_penalty=2.5,\n",
    "                  length_penalty=1.0,\n",
    "                  early_stopping=True\n",
    "                  )\n",
    "              preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "              target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "              if _%10==0:\n",
    "                  console.print(f'Completed {_}')\n",
    "\n",
    "              predictions.extend(preds)\n",
    "              actuals.extend(target)\n",
    "        return predictions, actuals\n",
    "\n",
    "    def T5Trainer(dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\"):\n",
    "\n",
    "        \"\"\"\n",
    "        T5 trainer\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Set random seeds and deterministic pytorch for reproducibility\n",
    "        torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "        np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        # logging\n",
    "        console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "        # tokenzier for encoding the text\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "        # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
    "        # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "        model = model.to(device)\n",
    "\n",
    "        # logging\n",
    "        console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "        # Importing the raw dataset\n",
    "        dataframe = dataframe[[source_text, target_text]]\n",
    "        display_df(dataframe.head(2))\n",
    "\n",
    "        # Creation of Dataset and Dataloader\n",
    "        # Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
    "        train_size = 0.8\n",
    "        train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
    "        val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "        train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "        console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "        console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "        console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "        # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "        training_set = YourDataSetClass(\n",
    "            train_dataset,\n",
    "            tokenizer,\n",
    "            model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "            model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "            source_text,\n",
    "            target_text,\n",
    "        )\n",
    "        val_set = YourDataSetClass(\n",
    "            val_dataset,\n",
    "            tokenizer,\n",
    "            model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "            model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "            source_text,\n",
    "            target_text,\n",
    "        )\n",
    "\n",
    "        # Defining the parameters for creation of dataloaders\n",
    "        train_params = {\n",
    "            \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "            \"shuffle\": True,\n",
    "            \"num_workers\": 0,\n",
    "        }\n",
    "\n",
    "        val_params = {\n",
    "            \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
    "            \"shuffle\": False,\n",
    "            \"num_workers\": 0,\n",
    "        }\n",
    "\n",
    "        # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "        training_loader = DataLoader(training_set, **train_params)\n",
    "        val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "        # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
    "        )\n",
    "\n",
    "        # Training loop\n",
    "        console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
    "\n",
    "        for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "            train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "        console.log(f\"[Saving Model]...\\n\")\n",
    "        # Saving the model after training\n",
    "        path = os.path.join(output_dir, \"model_files\")\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "\n",
    "        # evaluating test dataset\n",
    "        console.log(f\"[Initiating Validation]...\\n\")\n",
    "        for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "            predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "            final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
    "            final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n",
    "\n",
    "        console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
    "\n",
    "        console.log(f\"[Validation Completed.]\\n\")\n",
    "        console.print(\n",
    "            f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\"\n",
    "        )\n",
    "        console.print(\n",
    "            f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n",
    "        )\n",
    "        console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}